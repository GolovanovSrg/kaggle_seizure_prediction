{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 3: Tesla M2075 (CNMeM is enabled with initial size: 80.0% of memory, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.cross_validation import train_test_split as sk_split, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, GRU, Masking, BatchNormalization, GlobalAveragePooling1D, GaussianDropout,\\\n",
    "                         TimeDistributed, GlobalMaxPooling1D, GaussianNoise, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.optimizers import Adam, Nadam\n",
    "\n",
    "rnd_state = 2016\n",
    "%matplotlib inline\n",
    "\n",
    "def cross_validation(x, y, kfold, algo):\n",
    "    auc = []\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    x_mat = x.as_matrix()\n",
    "    y_mat = y.as_matrix()\n",
    "    \n",
    "    for train_index, test_index in kfold:\n",
    "        x_train, x_test = x_mat[train_index], x_mat[test_index]\n",
    "        y_train, y_test = y_mat[train_index], y_mat[test_index]\n",
    "        \n",
    "        x_train = scaler.fit_transform(x_train)\n",
    "        x_test = scaler.transform(x_test)\n",
    "        \n",
    "        algo.fit(x_train, y_train)\n",
    "        pred = algo.predict_proba(x_test)[:, 1]\n",
    "        \n",
    "        current_auc = roc_auc_score(y_test, pred)\n",
    "        auc.append(current_auc)\n",
    "        \n",
    "        print(\"current auc = {}\" .format(current_auc))\n",
    "    \n",
    "    mean_auc = np.mean(auc)\n",
    "    print(\"mean auc = {0}\" .format(mean_auc))\n",
    "    \n",
    "    return mean_auc\n",
    "    \n",
    "def plot_series(series):\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.bar(range(len(series)), series)\n",
    "    plt.xticks(range(len(series)), series.index);\n",
    "    plt.show()\n",
    "    \n",
    "def load_data(file_path):\n",
    "    print(\"Loading \" + file_path)\n",
    "    data = pd.read_csv(file_path, index_col=False)\n",
    "    data.interpolate(inplace=True, method ='slinear', limit_direction='both')\n",
    "    data.fillna(0.0, inplace=True)\n",
    "    \n",
    "    var = data.var(axis=0)\n",
    "    zvar_cols = list(var[var < 1e-9].index)\n",
    "    if 'class' in zvar_cols:\n",
    "        zvar_cols.remove('class')\n",
    "    data = data.drop(zvar_cols, axis=1)\n",
    "    if zvar_cols:\n",
    "        print(\"Columns {} haves zero variance.\\n\" .format(zvar_cols))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def train_test_split(data, test_frac):\n",
    "    is_target_cls = (data['class'] == 1)\n",
    "    \n",
    "    test_cols_1 = data[is_target_cls]['file'].drop_duplicates()\n",
    "    n_test_cols_1 = int(test_frac * len(test_cols_1))\n",
    "    pos_1 = np.random.randint(0, len(test_cols_1) - n_test_cols_1)\n",
    "    test_cols_1 = test_cols_1[pos_1 : pos_1 + n_test_cols_1]\n",
    "        \n",
    "    test_cols_0 = data[~is_target_cls]['file'].drop_duplicates()\n",
    "    n_test_cols_0 = int(test_frac * len(test_cols_0))\n",
    "    pos_0 = np.random.randint(0, len(test_cols_0) - n_test_cols_0)\n",
    "    test_cols_0 = test_cols_0[pos_0 : pos_0 + n_test_cols_0]\n",
    "        \n",
    "    test_cols = test_cols_1.append(test_cols_0)\n",
    "    \n",
    "    is_test = data['file'].isin(test_cols)\n",
    "    \n",
    "    return data[~is_test], data[is_test]\n",
    "    \n",
    "def train_test_validation_split(data_list, test_frac, val_frac):\n",
    "    train = []\n",
    "    test = []\n",
    "    val = []\n",
    "    \n",
    "    for data in data_list:\n",
    "        train_data, test_data = train_test_split(data, test_frac)\n",
    "        train_data, val_data = train_test_split(train_data, val_frac)\n",
    "        \n",
    "        train.append(train_data)\n",
    "        test.append(test_data)\n",
    "        val.append(val_data)\n",
    "        \n",
    "    return pd.concat(train, ignore_index=True), \\\n",
    "           pd.concat(test, ignore_index=True), \\\n",
    "           pd.concat(val, ignore_index=True)\n",
    "    \n",
    "def seq_transform(x_data, y_data, info_data, seq_len=10):    \n",
    "    X = []\n",
    "    Y = []\n",
    "    info_idxs = []\n",
    "    for i in range(0, x_data.shape[0] - seq_len + 1, seq_len):\n",
    "        X.append(x_data[i : i + seq_len])\n",
    "        info_idxs.append(i + seq_len - 1)\n",
    "        if y_data is not None:\n",
    "            Y.append([y_data[i + seq_len - 1]])\n",
    "        \n",
    "    return np.array(X), \\\n",
    "           np.array(Y), \\\n",
    "           info_data.ix[info_data.index[info_idxs], :]\n",
    "    \n",
    "def feats_target_info_split(data):\n",
    "    cols = list(filter(feats_predicat, data.columns))\n",
    "    \n",
    "    feats = data[cols]\n",
    "    target = data['class'] if 'class' in data.columns else None\n",
    "    info = data[['file', 'channel']]\n",
    "    \n",
    "    return feats, target, info\n",
    "\n",
    "def drop_zero(data):\n",
    "    return data[data['is_zero'] == 0]\n",
    "\n",
    "def repeat_target(x_data, y_data, x):\n",
    "    is_target = (y_data == 1).flatten()\n",
    "    x_list = [x_data] + [x_data[is_target]] * x\n",
    "    y_list = [y_data] + [y_data[is_target]] * x\n",
    "\n",
    "    new_x_train = np.vstack(tuple(x_list))\n",
    "    new_y_train = np.vstack(tuple(y_list))\n",
    "    \n",
    "    return new_x_train, new_y_train\n",
    "\n",
    "def augumentation_seq(data, n_obj):\n",
    "    seq_len = data.shape[1]\n",
    "    vec_len = data.shape[2]\n",
    "    res_len = data.shape[0] * n_obj\n",
    "    result = np.zeros((res_len, seq_len, vec_len), dtype=np.float32)\n",
    "    \n",
    "    pos = -1\n",
    "    seq = np.zeros((seq_len, vec_len))\n",
    "    for i in range(data.shape[0]):\n",
    "        objs = np.random.randint(0, data.shape[0] - 1, size=n_obj)\n",
    "        for j in objs:\n",
    "            if i != j:\n",
    "                pos += 1\n",
    "                seq[: seq_len // 2, :] = data[i, : seq_len // 2, :]\n",
    "                seq[seq_len // 2 :, :] = data[i, seq_len // 2 :, :]\n",
    "                result[pos, :, :] = seq\n",
    "                \n",
    "    \n",
    "    return result\n",
    "\n",
    "def gen_seq(x_data, y_data, batch_size):\n",
    "    seq_len = x_data.shape[1]\n",
    "    vec_len = x_data.shape[2]\n",
    "    \n",
    "    is_target = (y_data == 1).flatten()\n",
    "    \n",
    "    batch = np.zeros((batch_size, seq_len, vec_len), dtype=np.float32)\n",
    "    \n",
    "    target_data = x_data[is_target, :, :]\n",
    "    non_target_data = x_data[~is_target, :, :]\n",
    "    while True:\n",
    "        \n",
    "        target1 = np.random.randint(0, target_data.shape[0] - 1, size=batch_size // 2)\n",
    "        target2 = np.random.randint(0, target_data.shape[0] - 1, size=batch_size // 2)\n",
    "        \n",
    "        if np.random.rand() < 0.5:\n",
    "            batch[:batch_size // 2, :, :] = np.concatenate((target_data[target1, : seq_len // 2, :],\n",
    "                                                            target_data[target2, seq_len // 2 :, :]), axis=1)\n",
    "        else:\n",
    "            batch[:batch_size // 2, :, :] = target_data[target1, :, :]\n",
    "        \n",
    "        non_target1 = np.random.randint(0, non_target_data.shape[0] - 1, size=batch_size // 2)\n",
    "        non_target2 = np.random.randint(0, non_target_data.shape[0] - 1, size=batch_size // 2)\n",
    "        \n",
    "        if np.random.rand() < 0.5:\n",
    "            batch[batch_size // 2:, :, :] = np.concatenate((non_target_data[non_target1, : seq_len // 2, :],\n",
    "                                                        non_target_data[non_target2, seq_len // 2 :, :]), axis=1)\n",
    "        else:\n",
    "            batch[batch_size // 2:, :, :] = non_target_data[non_target1, :, :]\n",
    "        \n",
    "        labels = np.vstack((np.ones((batch_size // 2, 1)), \n",
    "                            np.zeros((batch_size // 2, 1))))\n",
    "        \n",
    "        perm = np.random.permutation(batch_size)\n",
    "        \n",
    "        b_tuple = (batch[perm, :, :].copy(), labels[perm].copy())\n",
    "        yield b_tuple\n",
    "        \n",
    "                \n",
    "\n",
    "feats_predicat = lambda x: \\\n",
    "                        x == 'max_mag_freq' or \\\n",
    "                        x == 'pitch_salience' or \\\n",
    "                        x == 'flatnessSFX' or \\\n",
    "                        x == 'strong_peak' or \\\n",
    "                        x == 'zcr' or \\\n",
    "                        x == 'percspread' or \\\n",
    "                        x == 'percsharpness' or \\\n",
    "                        x == 'flux' or \\\n",
    "                        x == 'roll_off' or \\\n",
    "                        x == 'kurtosis' or \\\n",
    "                        x.startswith(\"freq_energy\") or \\\n",
    "                        x == 'compl' or \\\n",
    "                        x == 'leq' or \\\n",
    "                        x.startswith(\"gfcc\") or \\\n",
    "                        x.startswith(\"ref\") or \\\n",
    "                        x.startswith(\"obsi\") or \\\n",
    "                        x.startswith(\"lpc\") or \\\n",
    "                        x == 'larm' or \\\n",
    "                        x == 'geo_mean' or \\\n",
    "                        x.startswith(\"lsf\") or \\\n",
    "                        x == 'corr' or \\\n",
    "                        x == 'skew' or \\\n",
    "                        x == 'hfc' or \\\n",
    "                        x.startswith(\"obsir\") or \\\n",
    "                        x == 'loudness' or \\\n",
    "                        x == 'derAvAfterMax' or \\\n",
    "                        x.startswith(\"distr\") or \\\n",
    "                        x == 'min_to_total' or \\\n",
    "                        x == 'maxDerBeforeMax' or \\\n",
    "                        x == 'max_to_total' or \\\n",
    "                        x == 'specflat' or \\\n",
    "                        x == 'specslope' or \\\n",
    "                        x.startswith(\"moment\") or \\\n",
    "                        x == 'entropy' or \\\n",
    "                        x == 'rms' or \\\n",
    "                        x == 'spec_cent' or \\\n",
    "                        x.startswith('channel')\n",
    "                        #x.startswith('order')# or \\\n",
    "                        #x.startswith('log_freq')\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../data/feats_1.csv\n",
      "Columns ['specslope'] haves zero variance.\n",
      "\n",
      "Loading ../data/feats_2.csv\n",
      "Columns ['specslope'] haves zero variance.\n",
      "\n",
      "Loading ../data/feats_3.csv\n",
      "Columns ['specslope'] haves zero variance.\n",
      "\n",
      "data_1 shape = (114768, 169)\n",
      "data_2 shape = (291888, 169)\n",
      "data_3 shape = (310752, 169)\n"
     ]
    }
   ],
   "source": [
    "data_path_1 = r\"../data/feats_1.csv\"\n",
    "data_path_2 = r\"../data/feats_2.csv\"\n",
    "data_path_3 = r\"../data/feats_3.csv\"\n",
    "\n",
    "data_1 = load_data(data_path_1)\n",
    "data_2 = load_data(data_path_2)\n",
    "data_3 = load_data(data_path_3)\n",
    "\n",
    "print(\"data_1 shape = {}\" .format(data_1.shape))\n",
    "print(\"data_2 shape = {}\" .format(data_2.shape))\n",
    "print(\"data_3 shape = {}\" .format(data_3.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape = (582048, 177)\n",
      "test shape = (71280, 177)\n",
      "val shape = (64080, 177)\n"
     ]
    }
   ],
   "source": [
    "test_frac = 0.1\n",
    "val_frac = 0.1\n",
    "\n",
    "data_list = [data_1, data_2, data_3]\n",
    "\n",
    "categorical_cols = ['channel']\n",
    "\n",
    "train, test, val = train_test_validation_split(data_list, test_frac, val_frac)\n",
    "x_train, y_train, info_train = feats_target_info_split(train)\n",
    "x_test, y_test, info_test = feats_target_info_split(test)\n",
    "x_val, y_val, info_val = feats_target_info_split(val)\n",
    "\n",
    "x_train_categor = pd.get_dummies(x_train[categorical_cols], columns=categorical_cols, drop_first=True)\n",
    "x_test_categor = pd.get_dummies(x_test[categorical_cols], columns=categorical_cols, drop_first=True)\n",
    "x_val_categor = pd.get_dummies(x_val[categorical_cols], columns=categorical_cols, drop_first=True)\n",
    "\n",
    "cols = list(filter(lambda x: x not in categorical_cols, x_train.columns))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train[cols])\n",
    "x_test = scaler.transform(x_test[cols])\n",
    "x_val = scaler.transform(x_val[cols])\n",
    "\n",
    "x_train = np.hstack([x_train, x_train_categor.as_matrix()])\n",
    "x_test = np.hstack([x_test, x_test_categor.as_matrix()])\n",
    "x_val = np.hstack([x_val, x_val_categor.as_matrix()])\n",
    "\n",
    "#x_train, y_train = repeat_target(x_train, y_train.reshape(-1, 1), 2)\n",
    "\n",
    "print(\"train shape = {}\" .format(x_train.shape))\n",
    "print(\"test shape = {}\" .format(x_test.shape))\n",
    "print(\"val shape = {}\" .format(x_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "activation = 'tanh'\n",
    "\n",
    "net = Sequential()\n",
    "net.add(Dense(128, input_dim=x_train.shape[1], init='he_normal', activation=activation))\n",
    "#net.add(Dropout(0.2))\n",
    "net.add(Dense(64, init='he_normal', activation=activation))\n",
    "#net.add(Dropout(0.2))\n",
    "net.add(Dense(32, init='he_normal', activation=activation))\n",
    "#net.add(Dropout(0.2))\n",
    "net.add(Dense(1, input_dim=x_train.shape[1], init='he_normal', activation='sigmoid'))\n",
    "opt = Adam()\n",
    "net.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "\n",
    "checkpoint = ModelCheckpoint('../net/best_net.hdf5', \n",
    "                              monitor='val_loss', \n",
    "                              verbose=0, \n",
    "                              save_best_only=True, \n",
    "                              mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net.load_weights(\"../net/best_net.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 582048 samples, validate on 64080 samples\n",
      "Epoch 1/10000\n",
      "582048/582048 [==============================] - 6s - loss: 0.3689 - val_loss: 0.3579\n",
      "Epoch 2/10000\n",
      "582048/582048 [==============================] - 6s - loss: 0.3491 - val_loss: 0.3530\n",
      "Epoch 3/10000\n",
      "582048/582048 [==============================] - 6s - loss: 0.3398 - val_loss: 0.3483\n",
      "Epoch 4/10000\n",
      "105472/582048 [====>.........................] - ETA: 5s - loss: 0.3350"
     ]
    }
   ],
   "source": [
    "net.fit(x_train, y_train, \n",
    "        validation_data=(x_val, y_val), \n",
    "        shuffle=True,\n",
    "        callbacks=[checkpoint],\n",
    "        batch_size=256, \n",
    "        nb_epoch=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = net.evaluate(x_test, y_test)\n",
    "print(\"\\nloss = {}\" .format(score))\n",
    "\n",
    "pred = net.predict(x_test)\n",
    "current_auc = roc_auc_score(y_test, pred)\n",
    "print(\"auc = {}\" .format(current_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(pred[y_test.reshape(-1, 1) == 1])\n",
    "plt.show()\n",
    "plt.hist(pred[y_test.reshape(-1, 1) == 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights_file = \"../net/net1.hdf5\"\n",
    "net.save_weights(weights_file, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_frac = 0.1\n",
    "val_frac = 0.2\n",
    "\n",
    "data_list = [data_1, data_2, data_3]\n",
    "\n",
    "train, test, val = train_test_validation_split(data_list, test_frac, val_frac)\n",
    "x_train, y_train, info_train = feats_target_info_split(train)\n",
    "x_test, y_test, info_test = feats_target_info_split(test)\n",
    "x_val, y_val, info_val = feats_target_info_split(val)\n",
    "\n",
    "categorical_cols = ['channel']\n",
    "'''x_train_categor = pd.get_dummies(x_train[categorical_cols], columns=categorical_cols, drop_first=True)\n",
    "x_test_categor = pd.get_dummies(x_test[categorical_cols], columns=categorical_cols, drop_first=True)\n",
    "x_val_categor = pd.get_dummies(x_val[categorical_cols], columns=categorical_cols, drop_first=True)'''\n",
    "\n",
    "cols = list(filter(lambda x: x not in categorical_cols, x_train.columns))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train[cols])\n",
    "x_test = scaler.transform(x_test[cols])\n",
    "x_val = scaler.transform(x_val[cols])\n",
    "\n",
    "'''x_train = np.hstack([x_train, x_train_categor.as_matrix()])\n",
    "x_test = np.hstack([x_test, x_test_categor.as_matrix()])\n",
    "x_val = np.hstack([x_val, x_val_categor.as_matrix()])'''\n",
    "\n",
    "x_train, y_train, info_train = seq_transform(x_train, y_train, info_train)\n",
    "x_test, y_test, info_test = seq_transform(x_test, y_test, info_test)\n",
    "x_val, y_val, info_val = seq_transform(x_val, y_val, info_val)\n",
    "\n",
    "print(\"train shape = {}\" .format(x_train.shape))\n",
    "print(\"test shape = {}\" .format(x_test.shape))\n",
    "print(\"val shape = {}\" .format(x_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_target = (y_train == 1).flatten()\n",
    "\n",
    "aug_x_train_target = augumentation_seq(x_train[is_target, :, :], 3)\n",
    "aug_y_train_target = np.ones((aug_x_train_target.shape[0], 1), dtype=np.float32)\n",
    "\n",
    "aug_x_train = augumentation_seq(x_train[~is_target, :, :], 0)\n",
    "aug_y_train = np.zeros((aug_x_train.shape[0], 1), dtype=np.float32)\n",
    "\n",
    "\n",
    "x_train = np.vstack((x_train, aug_x_train_target, aug_x_train))\n",
    "y_train = np.vstack((y_train, aug_y_train_target, aug_y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "activation = 'tanh'\n",
    "inner_activation = 'tanh'\n",
    "initialize = 'he_normal'\n",
    "\n",
    "net = Sequential()\n",
    "#net.add(TimeDistributed(GaussianNoise(0.3), input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "#net.add(TimeDistributed(Dense(512, init=initialize, activation=activation), input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "#net.add(TimeDistributed(Dropout(0.1)))\n",
    "net.add(TimeDistributed(Dense(256, init=initialize, activation=activation), input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "net.add(TimeDistributed(Dropout(0.1)))\n",
    "net.add(LSTM(128, return_sequences=True,\n",
    "             input_shape=(x_train.shape[1], x_train.shape[2]), \n",
    "             init=initialize, activation=activation, dropout_W=0.1, dropout_U=0.1, \n",
    "             inner_activation=inner_activation))\n",
    "net.add(GlobalAveragePooling1D())\n",
    "net.add(Dense(1, init=initialize, activation='sigmoid'))\n",
    "net.compile(loss='binary_crossentropy', optimizer='nadam')\n",
    "\n",
    "checkpoint = ModelCheckpoint('../net/best_net_rnn.hdf5', \n",
    "                          monitor='val_loss', \n",
    "                          verbose=0, \n",
    "                          save_best_only=True, \n",
    "                          mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net.load_weights(\"../net/best_net_rnn.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net.fit(x_train, y_train, \n",
    "        validation_data=(x_val, y_val), \n",
    "        shuffle=True,\n",
    "        callbacks=[checkpoint],\n",
    "        batch_size=256, \n",
    "        nb_epoch=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "generator = gen_seq(x_train, y_train, batch_size)\n",
    "\n",
    "net.fit_generator(generator, validation_data=(x_val, y_val), \n",
    "                  samples_per_epoch=50 * batch_size, \n",
    "                  nb_epoch=1000, \n",
    "                  callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = net.evaluate(x_test, y_test)\n",
    "print(\"\\nloss = {}\" .format(score))\n",
    "\n",
    "pred = net.predict(x_test)\n",
    "current_auc = roc_auc_score(y_test, pred)\n",
    "print(\"auc = {}\" .format(current_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(pred[y_test == 1])\n",
    "plt.show()\n",
    "plt.hist(pred[y_test == 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_file = \"./rnn.hdf5\"\n",
    "net.save_weights(weights_file, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_frac = 0.1\n",
    "val_frac = 0.2\n",
    "\n",
    "data_list = [data_1, data_2, data_3]\n",
    "\n",
    "train, test, val = train_test_validation_split(data_list, test_frac, val_frac)\n",
    "x_train, y_train, info_train = feats_target_info_split(train)\n",
    "x_test, y_test, info_test = feats_target_info_split(test)\n",
    "x_val, y_val, info_val = feats_target_info_split(val)\n",
    "\n",
    "categorical_cols = ['channel']\n",
    "x_train_categor = pd.get_dummies(x_train[categorical_cols], columns=categorical_cols, drop_first=True)\n",
    "x_test_categor = pd.get_dummies(x_test[categorical_cols], columns=categorical_cols, drop_first=True)\n",
    "x_val_categor = pd.get_dummies(x_val[categorical_cols], columns=categorical_cols, drop_first=True)\n",
    "\n",
    "cols = list(filter(lambda x: x not in categorical_cols, x_train.columns))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train[cols])\n",
    "x_test = scaler.transform(x_test[cols])\n",
    "x_val = scaler.transform(x_val[cols])\n",
    "\n",
    "x_train = np.hstack([x_train, x_train_categor.as_matrix()])\n",
    "x_test = np.hstack([x_test, x_test_categor.as_matrix()])\n",
    "x_val = np.hstack([x_val, x_val_categor.as_matrix()])\n",
    "\n",
    "#x_train, y_train = repeat_target(x_train, y_train.reshape(-1, 1), 4)\n",
    "\n",
    "print(\"train shape = {}\" .format(x_train.shape))\n",
    "print(\"test shape = {}\" .format(x_test.shape))\n",
    "print(\"val shape = {}\" .format(x_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_gb = xgb.XGBClassifier(n_estimators=100,\n",
    "                           max_depth=4,\n",
    "                           learning_rate=0.3,\n",
    "                           objective='binary:logistic',\n",
    "                           max_delta_step=1,\n",
    "                           silent=False,\n",
    "                           seed=rnd_state,\n",
    "                           subsample=0.7,\n",
    "                           colsample_bytree=0.7,\n",
    "                           min_child_weight=9,\n",
    "                           scale_pos_weight=0.75,\n",
    "                           reg_lambda=0,\n",
    "                           nthread=12)\n",
    "\n",
    "xgb_gb.fit(x_train, y_train.ravel())\n",
    "\n",
    "pred = xgb_gb.predict_proba(x_val)[:, 1]\n",
    "current_auc = roc_auc_score(y_val, pred)\n",
    "print(\"auc = {}\" .format(current_auc))\n",
    "\n",
    "pred = xgb_gb.predict_proba(x_test)[:, 1]\n",
    "current_auc = roc_auc_score(y_test, pred)\n",
    "print(\"auc = {}\" .format(current_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.hist(pred.reshape(-1, 1)[y_test.reshape(-1, 1) == 1])\n",
    "plt.show()\n",
    "plt.hist(pred.reshape(-1, 1)[y_test.reshape(-1, 1) == 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "xgb.plot_importance(xgb_gb.booster())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100,\n",
    "                           criterion='entropy',\n",
    "                           max_depth=8,\n",
    "                           min_samples_split=10,\n",
    "                           min_samples_leaf=5,\n",
    "                           n_jobs=12,\n",
    "                            oob_score=True,\n",
    "                           random_state=rnd_state,\n",
    "                           verbose=0)\n",
    "    \n",
    "rf.fit(x_train, y_train.ravel())\n",
    "\n",
    "pred = rf.predict_proba(x_val)[:, 1]\n",
    "current_auc = roc_auc_score(y_val, pred)\n",
    "print(\"auc = {}\" .format(current_auc))\n",
    "\n",
    "pred = rf.predict_proba(x_test)[:, 1]\n",
    "current_auc = roc_auc_score(y_test, pred)\n",
    "print(\"auc = {}\" .format(current_auc))\n",
    "\n",
    "kfold = KFold(n=x_train.shape[0],\n",
    "              n_folds=5,\n",
    "              shuffle=True,\n",
    "              random_state=rnd_state)\n",
    "\n",
    "#auc_score = cross_validation(x_train, y_train, kfold, rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, y_train, info_train = feats_target_info_split(data)\n",
    "\n",
    "kfold = KFold(n=x_train.shape[0],\n",
    "              n_folds=5,\n",
    "              shuffle=True,\n",
    "              random_state=rnd_state)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100,\n",
    "                           criterion='entropy',\n",
    "                           max_depth=18,\n",
    "                           min_samples_split=2,\n",
    "                           min_samples_leaf=6,\n",
    "                           n_jobs=12,\n",
    "                           random_state=rnd_state)\n",
    "\n",
    "auc_score = cross_validation(x_train, y_train, kfold, rf)\n",
    "\n",
    "plot_series(pd.Series(rf.feature_importances_, index=x_train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data_path_1 = r\"../data/test_feats_1.csv\"\n",
    "test_data_path_2 = r\"../data/test_feats_2.csv\"\n",
    "test_data_path_3 = r\"../data/test_feats_3.csv\"\n",
    "\n",
    "test_data_1 = load_data(test_data_path_1)\n",
    "test_data_2 = load_data(test_data_path_2)\n",
    "test_data_3 = load_data(test_data_path_3)\n",
    "\n",
    "print(\"data_1 shape = {}\" .format(test_data_1.shape))\n",
    "print(\"data_2 shape = {}\" .format(test_data_2.shape))\n",
    "print(\"data_3 shape = {}\" .format(test_data_3.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data_1['id'] = 1\n",
    "test_data_2['id'] = 2\n",
    "test_data_3['id'] = 3\n",
    "\n",
    "test_data = test_data_1.append(test_data_2).append(test_data_3)\n",
    "\n",
    "uniq_files = np.unique(test_data['file'].as_matrix())\n",
    "file = test_data['file'].as_matrix().reshape((-1, 1))\n",
    "\n",
    "test_feats = feats_target_info_split(test_data)[0]\n",
    "test_feats_categor = pd.get_dummies(test_feats[categorical_cols], columns=categorical_cols, drop_first=True)\n",
    "test_feats = scaler.transform(test_feats[cols])\n",
    "test_feats = np.hstack([test_feats, test_feats_categor.as_matrix()])\n",
    "\n",
    "print(\"feats shape = {}\" .format(test_feats.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data_1['id'] = 1\n",
    "test_data_2['id'] = 2\n",
    "test_data_3['id'] = 3\n",
    "test_data = pd.concat([test_data_1, test_data_2, test_data_3], ignore_index=True)\n",
    "\n",
    "test_feats, test_target, test_info = feats_target_info_split(test_data)\n",
    "#test_feats_categor = pd.get_dummies(test_feats[categorical_cols], columns=categorical_cols, drop_first=True)\n",
    "test_feats = scaler.transform(test_feats[cols])\n",
    "#test_feats = np.hstack([test_feats, test_feats_categor.as_matrix()])\n",
    "test_feats, _, test_info = seq_transform(test_feats, test_target, test_info)\n",
    "\n",
    "file = test_info['file'].as_matrix().reshape((-1, 1))\n",
    "uniq_files = np.unique(file)\n",
    "\n",
    "print(\"feats shape = {}\" .format(test_feats.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#result = net.predict(test_feats)\n",
    "result = xgb_gb.predict_proba(test_feats)[:, 1].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_files = len(uniq_files)\n",
    "result_mat = np.zeros((n_files, 2), dtype=np.object)\n",
    "\n",
    "for i in tqdm(range(n_files)):\n",
    "    result_mat[i, 0] = uniq_files[i]\n",
    "    frames = result[file == uniq_files[i]]\n",
    "    if frames.shape[0] == 0:\n",
    "        result_mat[i, 1] = 0.0\n",
    "    else:\n",
    "        result_mat[i, 1] = gmean(frames)\n",
    "    \n",
    "res_data = pd.DataFrame(result_mat, columns=['File', 'Class'])\n",
    "res_data.to_csv('../result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res_data['Class'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
